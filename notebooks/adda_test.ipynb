{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/jvanvugt/pytorch-domain-adaptation/blob/master/adda.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import dotenv\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, RichProgressBar\n",
    "\n",
    "from inz.data.data_module import XBDDataModule\n",
    "from inz.data.event import Event, Tier3, Tier1\n",
    "from inz.models.baseline_module import BaselineModule\n",
    "from inz.util import get_loc_cls_weights, get_wandb_logger, show_masks_comparison\n",
    "from inz.xview2_strong_baseline.legacy.losses import ComboLoss\n",
    "from inz.xview2_strong_baseline.legacy.zoo.models import Res34_Unet_Double\n",
    "from inz.models.baseline_module import BaselineModule\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmetrics.functional.classification import multiclass_f1_score\n",
    "from torchmetrics.functional.classification import binary_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  # noqa: I001\n",
    "\n",
    "sys.path.append(\"inz/revgrad\")\n",
    "\n",
    "from utils import GradientReversal\n",
    "\n",
    "sys.path.append(\"inz/xview2_strong_baseline\")\n",
    "\n",
    "from legacy.zoo.models import Res34_Unet_Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    }
   ],
   "source": [
    "dotenv.load_dotenv()\n",
    "RANDOM_SEED = 123\n",
    "pl.seed_everything(RANDOM_SEED)\n",
    "device = torch.device(\"cuda\")\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 train batches, 18 val batches\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "dm1 = XBDDataModule(\n",
    "    path=Path(\"data/xBD_processed_512\"),\n",
    "    drop_unclassified_channel=True,\n",
    "    events={\n",
    "        Tier1: [\n",
    "            Event.hurricane_matthew,\n",
    "        ],\n",
    "    },\n",
    "    val_fraction=0.15,\n",
    "    test_fraction=0.0,\n",
    "    train_batch_size=BATCH_SIZE,\n",
    "    val_batch_size=BATCH_SIZE,\n",
    "    test_batch_size=BATCH_SIZE,\n",
    ")\n",
    "dm1.prepare_data()\n",
    "dm1.setup(\"fit\")\n",
    "dm2 = XBDDataModule(\n",
    "    path=Path(\"data/xBD_processed_512\"),\n",
    "    drop_unclassified_channel=True,\n",
    "    events={\n",
    "        Tier1: [\n",
    "            Event.palu_tsunami,\n",
    "        ],\n",
    "    },\n",
    "    val_fraction=0.15,\n",
    "    test_fraction=0.0,\n",
    "    train_batch_size=BATCH_SIZE,\n",
    "    val_batch_size=BATCH_SIZE,\n",
    "    test_batch_size=BATCH_SIZE,\n",
    ")\n",
    "dm2.prepare_data()\n",
    "dm2.setup(\"fit\")\n",
    "\n",
    "print(f\"{len(dm1.train_dataloader())} train batches, {len(dm1.val_dataloader())} val batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CKPT = \"/home/tomek/inz/inz/outputs/split_wind_test_hurricane_matthew_baseline/2024-11-06_05-02-23/checkpoints/experiment_name-0-epoch-79-step-9600-f1-0.601407-last.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using weights from ResNet34_Weights.IMAGENET1K_V1\n",
      "using weights from ResNet34_Weights.IMAGENET1K_V1\n"
     ]
    }
   ],
   "source": [
    "source_model = BaselineModule.load_from_checkpoint(\n",
    "    checkpoint_path=MODEL_CKPT,\n",
    "    model=Res34_Unet_Double(pretrained=True),\n",
    "    loss=ComboLoss(weights={\"dice\": 1, \"focal\": 1}),\n",
    "    optimizer_factory=partial(torch.optim.AdamW, lr=0.0002, weight_decay=1e-6),\n",
    "    scheduler_factory=partial(\n",
    "        torch.optim.lr_scheduler.MultiStepLR,\n",
    "        gamma=0.5,\n",
    "        milestones=[5, 11, 17, 23, 29, 33, 47, 50, 60, 70, 90, 110, 130, 150, 170, 180, 190],\n",
    "    ),\n",
    "    class_weights=torch.Tensor([0.01, 1.0, 9.0478803, 8.68207691, 12.9632271]),\n",
    ")\n",
    "\n",
    "target_model = BaselineModule.load_from_checkpoint(\n",
    "    checkpoint_path=MODEL_CKPT,\n",
    "    model=Res34_Unet_Double(pretrained=True),\n",
    "    loss=ComboLoss(weights={\"dice\": 1, \"focal\": 1}),\n",
    "    optimizer_factory=partial(torch.optim.AdamW, lr=0.0002, weight_decay=1e-6),\n",
    "    scheduler_factory=partial(\n",
    "        torch.optim.lr_scheduler.MultiStepLR,\n",
    "        gamma=0.5,\n",
    "        milestones=[5, 11, 17, 23, 29, 33, 47, 50, 60, 70, 90, 110, 130, 150, 170, 180, 190],\n",
    "    ),\n",
    "    class_weights=torch.Tensor([0.01, 1.0, 9.0478803, 8.68207691, 12.9632271]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, enc_layers: list[torch.nn.Module]) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1, self.conv2, self.conv3, self.conv4, self.conv5 = enc_layers\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     return (self._encode_once(x[:, :3, ...]), self._encode_once(x[:, 3:, ...]))\n",
    "    #     # return torch.cat([torch.flatten(y, start_dim=1) for y in outputs])\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.conv1(x)\n",
    "        enc2 = self.conv2(enc1)\n",
    "        enc3 = self.conv3(enc2)\n",
    "        enc4 = self.conv4(enc3)\n",
    "        enc5 = self.conv5(enc4)\n",
    "        return [enc1, enc2, enc3, enc4, enc5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, dec_layers: list[torch.nn.Module], outconv: torch.nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        (\n",
    "            self.conv6,\n",
    "            self.conv6_2,\n",
    "            self.conv7,\n",
    "            self.conv7_2,\n",
    "            self.conv8,\n",
    "            self.conv8_2,\n",
    "            self.conv9,\n",
    "            self.conv9_2,\n",
    "            self.conv10,\n",
    "        ) = dec_layers\n",
    "        self.res = outconv\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        enc1_1, enc2_1, enc3_1, enc4_1, enc5_1 = x1\n",
    "        enc1_2, enc2_2, enc3_2, enc4_2, enc5_2 = x2\n",
    "        return self._forward(enc1_1, enc2_1, enc3_1, enc4_1, enc5_1, enc1_2, enc2_2, enc3_2, enc4_2, enc5_2)\n",
    "\n",
    "    def _forward(self, enc1_1, enc2_1, enc3_1, enc4_1, enc5_1, enc1_2, enc2_2, enc3_2, enc4_2, enc5_2):\n",
    "        output1 = self._decode_once(enc1_1, enc2_1, enc3_1, enc4_1, enc5_1)\n",
    "        output2 = self._decode_once(enc1_2, enc2_2, enc3_2, enc4_2, enc5_2)\n",
    "        return self.res(torch.cat([output1, output2], 1))\n",
    "\n",
    "    def _decode_once(self, enc1, enc2, enc3, enc4, enc5):\n",
    "        dec6 = self.conv6(F.interpolate(enc5, scale_factor=2))\n",
    "        dec6 = self.conv6_2(torch.cat([dec6, enc4], 1))\n",
    "        dec7 = self.conv7(F.interpolate(dec6, scale_factor=2))\n",
    "        dec7 = self.conv7_2(torch.cat([dec7, enc3], 1))\n",
    "        dec8 = self.conv8(F.interpolate(dec7, scale_factor=2))\n",
    "        dec8 = self.conv8_2(torch.cat([dec8, enc2], 1))\n",
    "        dec9 = self.conv9(F.interpolate(dec8, scale_factor=2))\n",
    "        dec9 = self.conv9_2(torch.cat([dec9, enc1], 1))\n",
    "        dec10 = self.conv10(F.interpolate(dec9, scale_factor=2))\n",
    "        return dec10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(model, requires_grad=True):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_encoder = (\n",
    "    EncoderWrapper(\n",
    "        enc_layers=[\n",
    "            source_model.model.conv1,\n",
    "            source_model.model.conv2,\n",
    "            source_model.model.conv3,\n",
    "            source_model.model.conv4,\n",
    "            source_model.model.conv5,\n",
    "        ]\n",
    "    )\n",
    "    .eval()\n",
    "    .to(device)\n",
    ")\n",
    "set_requires_grad(source_encoder, False)\n",
    "\n",
    "target_encoder = EncoderWrapper(\n",
    "    enc_layers=[\n",
    "        target_model.model.conv1,\n",
    "        target_model.model.conv2,\n",
    "        target_model.model.conv3,\n",
    "        target_model.model.conv4,\n",
    "        target_model.model.conv5,\n",
    "    ]\n",
    ").to(device)\n",
    "\n",
    "decoder = (\n",
    "    DecoderWrapper(\n",
    "        dec_layers=[\n",
    "            source_model.model.conv6,\n",
    "            source_model.model.conv6_2,\n",
    "            source_model.model.conv7,\n",
    "            source_model.model.conv7_2,\n",
    "            source_model.model.conv8,\n",
    "            source_model.model.conv8_2,\n",
    "            source_model.model.conv9,\n",
    "            source_model.model.conv9_2,\n",
    "            source_model.model.conv10,\n",
    "        ],\n",
    "        outconv=source_model.model.res,\n",
    "    )\n",
    "    .eval()\n",
    "    .to(device)\n",
    ")\n",
    "set_requires_grad(decoder, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using weights from ResNet34_Weights.IMAGENET1K_V1\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.modules import Module\n",
    "\n",
    "REVERSAL_LAMBDA = 0.2\n",
    "\n",
    "\n",
    "class DecoderDiscriminator(DecoderWrapper):\n",
    "    def __init__(self, dec_layers: list[Module], outconv: Module) -> None:\n",
    "        super().__init__(dec_layers, outconv)\n",
    "        self.res = nn.Sequential(outconv, nn.Flatten(), nn.Linear(5 * 512**2, 1))\n",
    "\n",
    "\n",
    "discriminator_base = BaselineModule(\n",
    "    model=Res34_Unet_Double(pretrained=True),\n",
    "    loss=ComboLoss(weights={\"dice\": 1, \"focal\": 1}),\n",
    "    optimizer_factory=partial(torch.optim.AdamW, lr=0.0002, weight_decay=1e-6),\n",
    "    scheduler_factory=partial(\n",
    "        torch.optim.lr_scheduler.MultiStepLR,\n",
    "        gamma=0.5,\n",
    "        milestones=[5, 11, 17, 23, 29, 33, 47, 50, 60, 70, 90, 110, 130, 150, 170, 180, 190],\n",
    "    ),\n",
    "    class_weights=torch.Tensor([0.1, 1.0, 9.0478803, 8.68207691, 12.9632271]),\n",
    ")\n",
    "\n",
    "discriminator = DecoderDiscriminator(\n",
    "    dec_layers=[\n",
    "        discriminator_base.model.conv6,\n",
    "        discriminator_base.model.conv6_2,\n",
    "        discriminator_base.model.conv7,\n",
    "        discriminator_base.model.conv7_2,\n",
    "        discriminator_base.model.conv8,\n",
    "        discriminator_base.model.conv8_2,\n",
    "        discriminator_base.model.conv9,\n",
    "        discriminator_base.model.conv9_2,\n",
    "        discriminator_base.model.conv10,\n",
    "    ],\n",
    "    outconv=discriminator_base.model.res,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomek/inz/inz/.venv/lib/python3.11/site-packages/torch/_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "source_loader = dm1.train_dataloader()\n",
    "target_loader = dm2.train_dataloader()\n",
    "\n",
    "\n",
    "optim_target_encoder = torch.optim.AdamW(list(target_encoder.parameters()) + list(target_encoder.parameters()), lr=0.00001, weight_decay=1e-7)\n",
    "# scheduler_target_encoder = torch.optim.lr_scheduler.MultiStepLR(\n",
    "#     optim_target_encoder, gamma=0.5, milestones=[5, 11, 17, 23, 29, 33, 47, 50, 60, 70, 90, 110, 130, 150, 170, 180, 190]\n",
    "# )\n",
    "\n",
    "optim_discriminator = torch.optim.AdamW(list(discriminator.parameters()) + list(discriminator.parameters()), lr=0.00001, weight_decay=1e-7)\n",
    "# scheduler_discriminator = torch.optim.lr_scheduler.MultiStepLR(\n",
    "#     optim_discriminator, gamma=0.5, milestones=[5, 11, 17, 23, 29, 33, 47, 50, 60, 70, 90, 110, 130, 150, 170, 180, 190]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO only use selected (deeper/shallower?) layers for feature regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1702.05464\n",
    "\n",
    "https://github.com/jvanvugt/pytorch-domain-adaptation/blob/master/adda.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe095b582c349e6af8a9a5ee72220fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 001: discriminator_loss=0.6898, discriminator_accuracy=0.6250\n",
      "STEP 002: discriminator_loss=1.0371, discriminator_accuracy=0.5000\n",
      "STEP 003: encoder_loss=0.3783\n",
      "STEP 004: encoder_loss=0.3850\n",
      "STEP 005: encoder_loss=0.3443\n",
      "STEP 006: encoder_loss=0.3265\n",
      "STEP 007: encoder_loss=0.3544\n",
      "STEP 008: encoder_loss=0.3290\n",
      "STEP 009: encoder_loss=0.3255\n",
      "STEP 010: encoder_loss=0.3317\n",
      "STEP 011: encoder_loss=0.3118\n",
      "STEP 012: encoder_loss=0.3190\n",
      "STEP 013: encoder_loss=0.3064\n",
      "STEP 014: encoder_loss=0.3410\n",
      "STEP 015: encoder_loss=0.2816\n",
      "STEP 016: encoder_loss=0.2927\n",
      "STEP 017: discriminator_loss=0.8584, discriminator_accuracy=0.5000\n",
      "STEP 018: discriminator_loss=0.7874, discriminator_accuracy=0.5000\n",
      "STEP 019: encoder_loss=0.6140\n",
      "STEP 020: encoder_loss=0.6223\n",
      "STEP 021: encoder_loss=0.5916\n",
      "STEP 022: encoder_loss=0.5940\n",
      "STEP 023: encoder_loss=0.5413\n",
      "STEP 024: encoder_loss=0.5475\n",
      "STEP 025: encoder_loss=0.5614\n",
      "STEP 026: encoder_loss=0.5390\n",
      "STEP 027: encoder_loss=0.5248\n",
      "STEP 028: encoder_loss=0.6017\n",
      "STEP 029: encoder_loss=0.5636\n",
      "STEP 030: encoder_loss=0.5405\n",
      "STEP 031: encoder_loss=0.5782\n",
      "STEP 032: encoder_loss=0.5419\n",
      "STEP 033: discriminator_loss=0.7374, discriminator_accuracy=0.5000\n",
      "STEP 034: discriminator_loss=0.6811, discriminator_accuracy=0.4375\n",
      "STEP 035: encoder_loss=1.1577\n",
      "STEP 036: encoder_loss=1.1509\n",
      "STEP 037: encoder_loss=1.1697\n",
      "STEP 038: encoder_loss=1.1520\n",
      "STEP 039: encoder_loss=1.1010\n",
      "STEP 040: encoder_loss=1.1154\n",
      "STEP 041: encoder_loss=1.0896\n",
      "STEP 042: encoder_loss=1.0608\n",
      "STEP 043: encoder_loss=1.1254\n",
      "STEP 044: encoder_loss=1.0461\n",
      "STEP 045: encoder_loss=1.0608\n",
      "STEP 046: encoder_loss=1.0213\n",
      "STEP 047: encoder_loss=1.0775\n",
      "STEP 048: encoder_loss=1.0811\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25c8f2fcda64124a5ce85468878509b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## EPOCH 001: encoder_loss=0.6666, discriminator_loss=0.7985\n",
      "discriminator_accuracy=0.5104\n",
      "target_f1=0.3829\n",
      "target_f1_class=tensor([9.5475e-01, 5.4677e-01, 2.4741e-05, 9.1625e-02, 3.2111e-01],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc74149d3c3747f794585926f22d03b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 009: discriminator_loss=0.7383, discriminator_accuracy=0.5000\n",
      "STEP 010: discriminator_loss=0.7469, discriminator_accuracy=0.5000\n",
      "STEP 011: encoder_loss=0.9859\n",
      "STEP 012: encoder_loss=0.9348\n",
      "STEP 013: encoder_loss=0.9578\n",
      "STEP 014: encoder_loss=0.8599\n",
      "STEP 015: encoder_loss=0.8443\n",
      "STEP 016: encoder_loss=0.9742\n",
      "STEP 017: encoder_loss=0.9150\n",
      "STEP 018: encoder_loss=0.8987\n",
      "STEP 019: encoder_loss=0.8642\n",
      "STEP 020: encoder_loss=0.8364\n",
      "STEP 021: encoder_loss=0.8477\n",
      "STEP 022: encoder_loss=0.8175\n",
      "STEP 023: encoder_loss=0.8033\n",
      "STEP 024: encoder_loss=0.7637\n",
      "STEP 025: discriminator_loss=0.7164, discriminator_accuracy=0.5000\n",
      "STEP 026: discriminator_loss=0.7207, discriminator_accuracy=0.4375\n",
      "STEP 027: encoder_loss=0.8518\n",
      "STEP 028: encoder_loss=0.7543\n",
      "STEP 029: encoder_loss=0.7619\n",
      "STEP 030: encoder_loss=0.6764\n",
      "STEP 031: encoder_loss=0.6982\n",
      "STEP 032: encoder_loss=0.7141\n",
      "STEP 033: encoder_loss=0.7161\n",
      "STEP 034: encoder_loss=0.7096\n",
      "STEP 035: encoder_loss=0.6665\n",
      "STEP 036: encoder_loss=0.7049\n",
      "STEP 037: encoder_loss=0.6652\n",
      "STEP 038: encoder_loss=0.6466\n",
      "STEP 039: encoder_loss=0.6879\n",
      "STEP 040: encoder_loss=0.6147\n",
      "STEP 041: discriminator_loss=0.7535, discriminator_accuracy=0.4375\n",
      "STEP 042: discriminator_loss=0.7146, discriminator_accuracy=0.6250\n",
      "STEP 043: encoder_loss=0.8089\n",
      "STEP 044: encoder_loss=0.8208\n",
      "STEP 045: encoder_loss=0.7652\n",
      "STEP 046: encoder_loss=0.8000\n",
      "STEP 047: encoder_loss=0.8363\n",
      "STEP 048: encoder_loss=0.8314\n",
      "STEP 049: encoder_loss=0.7989\n",
      "STEP 050: encoder_loss=0.7947\n",
      "STEP 051: encoder_loss=0.7396\n",
      "STEP 052: encoder_loss=0.7618\n",
      "STEP 053: encoder_loss=0.7330\n",
      "STEP 054: encoder_loss=0.7793\n",
      "STEP 055: encoder_loss=0.7504\n",
      "STEP 056: encoder_loss=0.6899\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5a2c4354fb460192de03b03ab1a366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 136\u001b[0m\n\u001b[1;32m    133\u001b[0m total_f1_cls_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 136\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_batches\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_img_pre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_mask_pre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_img_post\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_mask_post\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures_target_pre\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_img_pre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/inz/inz/.venv/lib/python3.11/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/inz/inz/.venv/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/inz/inz/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/inz/inz/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/inz/inz/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1285\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1287\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/inz/inz/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from itertools import islice\n",
    "\n",
    "EPOCHS = 40\n",
    "\n",
    "STEPS_ENC = 14\n",
    "STEPS_DISC = 2\n",
    "c_enc = 0\n",
    "c_disc = 0\n",
    "current = \"disc\"\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    batches = zip(source_loader, target_loader)\n",
    "    n_batches = min(len(source_loader), len(target_loader)) - 1\n",
    "\n",
    "    total_discriminator_loss = total_encoder_loss =  total_discriminator_accuracy = discriminator_steps = encoder_steps = 0\n",
    "\n",
    "    target_encoder.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    for step, ((source_batch), (target_batch)) in enumerate(\n",
    "        tqdm(islice(batches, n_batches), total=n_batches, position=0, leave=True), start=1\n",
    "    ):\n",
    "        s_img_pre, s_mask_pre, s_img_post, s_mask_post = source_batch\n",
    "        t_img_pre, t_mask_pre, t_img_post, t_mask_post = target_batch\n",
    "\n",
    "        img_source = torch.cat([s_img_pre, s_img_post], dim=1).to(device)\n",
    "        img_target = torch.cat([s_img_pre, s_img_post], dim=1).to(device)\n",
    "\n",
    "        if current == \"disc\":\n",
    "            # train discriminator\n",
    "\n",
    "            set_requires_grad(target_encoder, False)\n",
    "            set_requires_grad(discriminator, True)\n",
    "\n",
    "            domain_y = torch.cat([torch.ones(img_source.shape[0]), torch.zeros(img_target.shape[0])]).to(device)\n",
    "\n",
    "            features_source_pre = source_encoder(s_img_pre.to(device))\n",
    "            features_source_post = source_encoder(s_img_post.to(device))\n",
    "            features_target_pre = target_encoder(t_img_pre.to(device))\n",
    "            features_target_post = target_encoder(t_img_post.to(device))\n",
    "\n",
    "            features_source_flat = torch.cat(\n",
    "                (\n",
    "                    torch.cat([t.flatten(start_dim=1) for t in features_source_pre], dim=1),\n",
    "                    torch.cat([t.flatten(start_dim=1) for t in features_source_post], dim=1),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            features_target_flat = torch.cat(\n",
    "                (\n",
    "                    torch.cat([t.flatten(start_dim=1) for t in features_target_pre], dim=1),\n",
    "                    torch.cat([t.flatten(start_dim=1) for t in features_target_post], dim=1),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            features_cat = torch.cat((features_source_flat, features_target_flat))\n",
    "\n",
    "            domain_preds_source = discriminator(features_source_pre, features_source_post)\n",
    "            domain_preds_target = discriminator(features_target_pre, features_target_post)\n",
    "\n",
    "            domain_preds = torch.cat((domain_preds_source, domain_preds_target)).squeeze()\n",
    "\n",
    "            discriminator_loss = F.binary_cross_entropy_with_logits(domain_preds, domain_y)\n",
    "\n",
    "\n",
    "            optim_discriminator.zero_grad()\n",
    "            discriminator_loss.backward()\n",
    "            optim_discriminator.step()\n",
    "\n",
    "            total_discriminator_loss += discriminator_loss.item()\n",
    "            discriminator_accuracy = binary_accuracy(domain_preds, domain_y)\n",
    "            total_discriminator_accuracy += discriminator_accuracy\n",
    "\n",
    "            discriminator_steps += 1\n",
    "\n",
    "            c_disc += 1\n",
    "            if c_disc == STEPS_DISC:\n",
    "                c_disc = 0\n",
    "                current = \"enc\"\n",
    "\n",
    "            tqdm.write(f\"STEP {(epoch-1) * BATCH_SIZE + step:03d}: discriminator_loss={discriminator_loss.item():.4f}, discriminator_accuracy={discriminator_accuracy.item():.4f}\")\n",
    "\n",
    "        else:\n",
    "            # train encoder\n",
    "\n",
    "            set_requires_grad(target_encoder, True)\n",
    "            set_requires_grad(discriminator, False)\n",
    "\n",
    "            features_target_pre = target_encoder(t_img_pre.to(device))\n",
    "            features_target_post = target_encoder(t_img_post.to(device))\n",
    "            features_target_flat = torch.cat(\n",
    "                (\n",
    "                    torch.cat([t.flatten(start_dim=1) for t in features_target_pre], dim=1),\n",
    "                    torch.cat([t.flatten(start_dim=1) for t in features_target_post], dim=1),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            # flipped labels\n",
    "            domain_y = torch.ones(img_source.shape[0]).to(device)\n",
    "\n",
    "            domain_preds = discriminator(features_target_pre, features_target_post).squeeze()\n",
    "\n",
    "            encoder_loss = F.binary_cross_entropy_with_logits(domain_preds, domain_y)\n",
    "\n",
    "            optim_target_encoder.zero_grad()\n",
    "            encoder_loss.backward()\n",
    "            optim_target_encoder.step()\n",
    "\n",
    "            total_encoder_loss += encoder_loss.item()\n",
    "\n",
    "            encoder_steps += 1\n",
    "\n",
    "            c_enc += 1\n",
    "            if c_enc == STEPS_ENC:\n",
    "                c_enc = 0\n",
    "                current = \"disc\"\n",
    "\n",
    "            tqdm.write(f\"STEP {(epoch-1) * BATCH_SIZE + step:03d}: encoder_loss={encoder_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    # evaluate\n",
    "    target_encoder.eval()\n",
    "    discriminator.eval()\n",
    "\n",
    "    batches = zip(source_loader, target_loader)\n",
    "    n_batches = min(len(source_loader), len(target_loader)) - 1\n",
    "\n",
    "    total_f1_target = 0\n",
    "    total_f1_cls_target = torch.Tensor([0., 0., 0., 0., 0.]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, ((source_batch), (target_batch)) in enumerate(\n",
    "            tqdm(islice(batches, n_batches), total=n_batches, position=0, leave=True), start=1\n",
    "        ):\n",
    "            t_img_pre, t_mask_pre, t_img_post, t_mask_post = target_batch\n",
    "\n",
    "            features_target_pre = target_encoder(t_img_pre.to(device))\n",
    "            features_target_post = target_encoder(t_img_post.to(device))\n",
    "\n",
    "            preds_target = decoder(features_target_pre, features_target_post)\n",
    "            total_f1_target += multiclass_f1_score(preds_target.argmax(dim=1), t_mask_post.to(device).argmax(dim=1), num_classes=5)\n",
    "            total_f1_cls_target += multiclass_f1_score(preds_target.argmax(dim=1), t_mask_post.to(device).argmax(dim=1), num_classes=5, average='none')\n",
    "\n",
    "\n",
    "    mean_encoder_loss = total_encoder_loss / encoder_steps\n",
    "    mean_discriminator_loss = total_discriminator_loss / discriminator_steps\n",
    "    mean_f1_target = total_f1_target / n_batches\n",
    "    mean_f1_target_class = total_f1_cls_target / n_batches\n",
    "    mean_discriminator_accuracy = total_discriminator_accuracy / discriminator_steps\n",
    "    tqdm.write(\n",
    "        f\"############## EPOCH {epoch:03d}: encoder_loss={mean_encoder_loss:.4f}, discriminator_loss={mean_discriminator_loss:.4f}\\n\"\n",
    "        f\"discriminator_accuracy={mean_discriminator_accuracy:.4f}\\n\"\n",
    "        f\"target_f1={mean_f1_target:.4f}\\n\"\n",
    "        f\"target_f1_class={mean_f1_target_class}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
