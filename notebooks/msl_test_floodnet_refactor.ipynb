{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import dotenv\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, RichProgressBar\n",
    "\n",
    "from inz.data.data_module import XBDDataModule\n",
    "from inz.data.event import Event, Tier3, Tier1, Hold, Test\n",
    "from inz.models.baseline_module import BaselineModule\n",
    "from inz.util import get_loc_cls_weights, get_wandb_logger, show_masks_comparison\n",
    "from inz.xview2_strong_baseline.legacy.losses import ComboLoss\n",
    "from inz.xview2_strong_baseline.legacy.zoo.models import Res34_Unet_Double\n",
    "from inz.models.baseline_module import BaselineModule\n",
    "from inz.data.zipped_data_module import ZippedDataModule\n",
    "from inz.models.farseg_singlebranch_module import SingleBranchFarSeg, FarSegSingleBranchModule\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import draw_segmentation_masks, make_grid\n",
    "import numpy as np\n",
    "from torchmetrics.functional.classification import multiclass_f1_score, binary_f1_score\n",
    "from torchmetrics.functional.classification import binary_accuracy\n",
    "import simplecv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  # noqa: I001\n",
    "\n",
    "sys.path.append(\"inz/revgrad\")\n",
    "\n",
    "from utils import GradientReversal\n",
    "\n",
    "sys.path.append(\"inz/xview2_strong_baseline\")\n",
    "\n",
    "from legacy.zoo.models import Res34_Unet_Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    }
   ],
   "source": [
    "dotenv.load_dotenv()\n",
    "RANDOM_SEED = 123\n",
    "pl.seed_everything(RANDOM_SEED)\n",
    "device = torch.device(\"cuda\")\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inz.data.data_module_floodnet import FloodNetDataset, FloodNetModule\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "source_events = {\n",
    "    Tier1: [\n",
    "        Event.hurricane_florence,\n",
    "        Event.hurricane_harvey,\n",
    "        Event.hurricane_michael\n",
    "    ],\n",
    "    Test: [\n",
    "        Event.hurricane_florence,\n",
    "        Event.hurricane_harvey,\n",
    "        Event.hurricane_michael\n",
    "    ],\n",
    "    Hold: [\n",
    "        Event.hurricane_florence,\n",
    "        Event.hurricane_harvey,\n",
    "        Event.hurricane_michael\n",
    "    ],\n",
    "    Tier3: [\n",
    "        Event.nepal_flooding\n",
    "    ],\n",
    "}\n",
    "\n",
    "_dm_source = XBDDataModule(\n",
    "    path=Path(\"data/xBD_processed_512\"),\n",
    "    drop_unclassified_channel=True,\n",
    "    events=source_events,\n",
    "    val_fraction=0.0,\n",
    "    test_fraction=0.0,\n",
    "    train_batch_size=BATCH_SIZE,\n",
    "    val_batch_size=BATCH_SIZE,\n",
    "    test_batch_size=BATCH_SIZE,\n",
    ")\n",
    "_dm_target = FloodNetModule(\n",
    "    path=Path(\"data/floodnet_processed_512/FloodNet-Supervised_v1.0\"),\n",
    "    train_batch_size=BATCH_SIZE,\n",
    "    val_batch_size=BATCH_SIZE,\n",
    "    test_batch_size=BATCH_SIZE,\n",
    "    transform=T.Compose(\n",
    "        transforms=[\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomApply(\n",
    "                p=0.6, transforms=[T.RandomAffine(degrees=(-10, 10), scale=(0.9, 1.1), translate=(0.1, 0.1))]\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "dm = ZippedDataModule(\n",
    "    dm1=_dm_source,\n",
    "    dm2=_dm_target,\n",
    "    match_type=\"max\",\n",
    "    num_workers=2,\n",
    "    train_batch_size=BATCH_SIZE,\n",
    "    val_batch_size=BATCH_SIZE,\n",
    "    test_batch_size=BATCH_SIZE\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup(\"fit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = dm.train_dataloader()\n",
    "source_batch, target_batch = next(iter(dataloader))\n",
    "\n",
    "s_img_pre, s_mask_pre, s_img_post, s_mask_post = source_batch\n",
    "t_img_pre, t_mask_pre, t_img_post, t_mask_post = target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "\n",
    "\n",
    "# def show(imgs: list[torch.Tensor]):\n",
    "#     if not isinstance(imgs, list):\n",
    "#         imgs = [imgs]\n",
    "#     fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "#     for i, img in enumerate(imgs):\n",
    "#         img = img.detach()\n",
    "#         img = T.functional.to_pil_image(img)\n",
    "#         axs[0, i].imshow(np.asarray(img))\n",
    "#         axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = [12, 24]\n",
    "# colors_source = [\n",
    "#     (128, 128, 128),\n",
    "#     (0, 255, 0),\n",
    "#     (244, 255, 0),\n",
    "#     (255, 174, 0),\n",
    "#     (255, 0, 0),\n",
    "#     (255, 255, 255),\n",
    "# ]\n",
    "# show(\n",
    "#     [\n",
    "#         make_grid((s_img_post + 1) / 2, nrow=2),\n",
    "#         make_grid(\n",
    "#             [draw_segmentation_masks(((i + 1) * 127.5).to(torch.uint8), m, colors=colors_source, alpha=0.5) for i, m in zip(s_img_post, s_mask_post.to(torch.bool))], nrow=2\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "# colors_target = [\n",
    "#     (128, 128, 128),\n",
    "#     (0, 255, 0),\n",
    "#     (255, 0, 0),\n",
    "# ]\n",
    "# show(\n",
    "#     [\n",
    "#         make_grid((t_img_post + 1) / 2, nrow=2),\n",
    "#         make_grid(\n",
    "#             [draw_segmentation_masks(((i + 1) * 127.5).to(torch.uint8), m, colors=colors_target, alpha=0.5) for i, m in zip(t_img_post, t_mask_post.to(torch.bool))], nrow=2\n",
    "#         ),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simplecv.util.logger:ResNetEncoder: pretrained = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene_relation: on\n",
      "loss type: cosine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomek/inz/inz/.venv/lib/python3.11/site-packages/pytorch_lightning/utilities/parsing.py:199: Attribute 'msl_loss_module' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['msl_loss_module'])`.\n"
     ]
    }
   ],
   "source": [
    "from inz.models.baseline_singlebranch import SingleBranchBaselinePLModule, BaselineSingleBranchModule\n",
    "from inz.models.msl.msl_module_wrapper import FloodnetMslModuleWrapper\n",
    "from inz.models.msl.msl_loss import IW_MaxSquareloss\n",
    "from functools import partial\n",
    "\n",
    "CLASS_WEIGHTS = torch.Tensor([0.01, 1.0, 9.0478803, 8.68207691, 12.9632271]).to(device)\n",
    "\n",
    "# optimizer_factory=partial(torch.optim.AdamW, lr=0.0002, weight_decay=1e-6),\n",
    "# lr=0.0002, weight_decay=1e-6 works\n",
    "# lr=0.00005, weight_decay=1e-6 works better\n",
    "# consider early stopping? 0.0002 initially improved the score, but degraded before the batch ended\n",
    "# ^ degradation slower / not present with 0.00005\n",
    "OPTIM_FACTORY = partial(torch.optim.AdamW, lr=0.00005, weight_decay=1e-6)\n",
    "SCHED_FACTORY = partial(\n",
    "        torch.optim.lr_scheduler.MultiStepLR,\n",
    "        gamma=0.5,\n",
    "        milestones=[5, 11, 17, 23, 29, 33, 47, 50, 60, 70, 90, 110, 130, 150, 170, 180, 190],\n",
    "    )\n",
    "MODEL_CKPT = Path('/home/tomek/inz/inz/outputs/baseline_singlebranch_flood/latest_run/checkpoints/experiment_name-0-epoch-31-step-7616-challenge_score_safe-0.5146-best-challenge-score.ckpt')\n",
    "_model = SingleBranchBaselinePLModule.load_from_checkpoint(\n",
    "    checkpoint_path=MODEL_CKPT,\n",
    "    model=BaselineSingleBranchModule(pretrained=True),\n",
    "    loss=ComboLoss(weights={\"dice\": 1, \"focal\": 1}),\n",
    "    optimizer_factory=OPTIM_FACTORY,\n",
    "    scheduler_factory=SCHED_FACTORY,\n",
    "    class_weights=CLASS_WEIGHTS,\n",
    ").to(device)\n",
    "\n",
    "# MODEL_CKPT = Path('/home/tomek/inz/inz/saved_checkpoints/runs/farseg_single/2024-10-25_00-48-01/checkpoints/experiment_name-0-epoch-28-step-28275-challenge_score_safe-0.6489-best-challenge-score.ckpt')\n",
    "# _model = FarSegSingleBranchModule.load_from_checkpoint(\n",
    "#     checkpoint_path=MODEL_CKPT,\n",
    "#     model=SingleBranchFarSeg(\n",
    "#         n_classes=5,\n",
    "#         farseg_config={\n",
    "#             \"resnet_encoder\": {\n",
    "#                 \"resnet_type\": \"resnet50\",\n",
    "#                 \"include_conv5\": True,\n",
    "#                 \"batchnorm_trainable\": True,\n",
    "#                 \"pretrained\": True,\n",
    "#                 \"freeze_at\": 0,\n",
    "#                 # 8, 16 or 32\n",
    "#                 \"output_stride\": 32,\n",
    "#                 \"with_cp\": [False, False, False, False],\n",
    "#                 \"stem3_3x3\": False\n",
    "#             },\n",
    "#             \"fpn\": {\n",
    "#                 \"in_channels_list\": [256, 512, 1024, 2048],\n",
    "#                 \"out_channels\": 256,\n",
    "#                 \"conv_block\": simplecv.module.fpn.default_conv_block,\n",
    "#                 \"top_blocks\": None,\n",
    "#             },\n",
    "#             \"scene_relation\": {\n",
    "#                 \"in_channels\": 2048,\n",
    "#                 \"channel_list\": [256, 256, 256, 256],\n",
    "#                 \"out_channels\": 256\n",
    "#             },\n",
    "#             \"decoder\": {\n",
    "#                 \"in_channels\": 256,\n",
    "#                 \"out_channels\": 128,\n",
    "#                 \"in_feat_output_strides\": [4, 8, 16, 32],\n",
    "#                 \"out_feat_output_stride\": 4,\n",
    "#                 \"norm_fn\": torch.nn.BatchNorm2d,\n",
    "#                 \"num_groups_gn\": None\n",
    "#             },\n",
    "#             \"num_classes\": 5,\n",
    "#             \"loss\": {\n",
    "#                 \"cls_weight\": 1.0,\n",
    "#                 \"ignore_index\": 255\n",
    "#             },\n",
    "#             \"annealing_softmax_focalloss\": {\n",
    "#                 \"gamma\": 2.0,\n",
    "#                 \"max_step\": 10000,\n",
    "#                 \"annealing_type\": \"cosine\",\n",
    "#             }\n",
    "#         }\n",
    "#     ),\n",
    "#     optimizer_factory=OPTIM_FACTORY,\n",
    "#     scheduler_factory=SCHED_FACTORY,\n",
    "#     class_weights=CLASS_WEIGHTS,\n",
    "# ).to(device)\n",
    "\n",
    "model = FloodnetMslModuleWrapper(\n",
    "    pl_module=_model,\n",
    "    n_classes_target=3,\n",
    "    msl_loss_module=IW_MaxSquareloss(ignore_index=-1, num_class=3, ratio=0.2).to(device),\n",
    "    msl_lambda=0.2,\n",
    "    optimizer_factory=OPTIM_FACTORY,\n",
    "    scheduler_factory=SCHED_FACTORY,\n",
    "    target_conf_matrix_labels=(\"Background\", \"Non-flooded\", \"Flooded\")\n",
    ").to(device)\n",
    "\n",
    "# model.forward_target(torch.cat([torch.zeros_like(t_img_post), t_img_post], dim=1).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copy import deepcopy\n",
    "\n",
    "# wandb_logger_test_initial = get_wandb_logger(\n",
    "#     run_name=f\"delete-me-test-initial-{datetime.datetime.now().replace(microsecond=0).isoformat()}\",\n",
    "#     project=\"inz\",\n",
    "#     watch_model=True,\n",
    "#     watch_model_log_frequency=500,\n",
    "#     watch_model_model=model,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm_test_initial = deepcopy(dm)\n",
    "# dm_test_initial.prepare_data()\n",
    "# dm_test_initial.setup(\"test\")\n",
    "# dm_test_initial.test_dataloader = dm_test_initial.train_dataloader\n",
    "\n",
    "# trainer = pl.Trainer(\n",
    "#     accelerator=\"gpu\",\n",
    "#     max_epochs=1,\n",
    "#     precision=\"bf16-mixed\",\n",
    "#     deterministic=True,\n",
    "#     sync_batchnorm=True,\n",
    "#     callbacks=[\n",
    "#         pl.callbacks.RichProgressBar()\n",
    "#     ],\n",
    "#     logger=wandb_logger_test_initial\n",
    "# )\n",
    "\n",
    "# trainer.test(model=model, datamodule=dm_test_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtomasz-owienko-stud\u001b[0m (\u001b[33mtomasz-owienko-stud-warsaw-university-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/tomek/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tomek/inz/inz/wandb/run-20241207_195447-4hyijtbc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tomasz-owienko-stud-warsaw-university-of-technology/inz/runs/4hyijtbc' target=\"_blank\">delete-me-2024-12-07T19:54:45</a></strong> to <a href='https://wandb.ai/tomasz-owienko-stud-warsaw-university-of-technology/inz' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tomasz-owienko-stud-warsaw-university-of-technology/inz' target=\"_blank\">https://wandb.ai/tomasz-owienko-stud-warsaw-university-of-technology/inz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tomasz-owienko-stud-warsaw-university-of-technology/inz/runs/4hyijtbc' target=\"_blank\">https://wandb.ai/tomasz-owienko-stud-warsaw-university-of-technology/inz/runs/4hyijtbc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/tomek/inz/inz/.venv/lib/python3.11/site-packages/pytorch_lightning/utilities/parsing.py:44: attribute 'msl_loss_module' removed from hparams because it cannot be pickled\n",
      "/home/tomek/inz/inz/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name                    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ inner                   │ FarSegSingleBranchModule  │ 31.4 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ msl_loss                │ IW_MaxSquareloss          │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ f1_source               │ MulticlassF1Score         │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ f1_loc_source           │ BinaryF1Score             │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ f1_source_per_class     │ MulticlassF1Score         │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ f1_target               │ MulticlassF1Score         │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ f1_target_per_class     │ MulticlassF1Score         │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span>│ f1_loc_target           │ BinaryF1Score             │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span>│ confusion_matrix_target │ MulticlassConfusionMatrix │      0 │\n",
       "└───┴─────────────────────────┴───────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ inner                   │ FarSegSingleBranchModule  │ 31.4 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ msl_loss                │ IW_MaxSquareloss          │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ f1_source               │ MulticlassF1Score         │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ f1_loc_source           │ BinaryF1Score             │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ f1_source_per_class     │ MulticlassF1Score         │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ f1_target               │ MulticlassF1Score         │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ f1_target_per_class     │ MulticlassF1Score         │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m7\u001b[0m\u001b[2m \u001b[0m│ f1_loc_target           │ BinaryF1Score             │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m8\u001b[0m\u001b[2m \u001b[0m│ confusion_matrix_target │ MulticlassConfusionMatrix │      0 │\n",
       "└───┴─────────────────────────┴───────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 31.4 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 31.4 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 125                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 31.4 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 31.4 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 125                                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a68bcb0f215476ebb79de9a792a3756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/inz/inz/.venv/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: 3 NaN \n",
       "values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/inz/inz/.venv/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: 3 NaN \n",
       "values found in confusion matrix have been replaced with zeros.\n",
       "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/tomek/inz/inz/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:54: Detected \n",
       "KeyboardInterrupt, attempting graceful shutdown...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/tomek/inz/inz/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:54: Detected \n",
       "KeyboardInterrupt, attempting graceful shutdown...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger = get_wandb_logger(\n",
    "    run_name=f\"delete-me-{datetime.datetime.now().replace(microsecond=0).isoformat()}\",\n",
    "    project=\"inz\",\n",
    "    watch_model=True,\n",
    "    watch_model_log_frequency=500,\n",
    "    watch_model_model=model,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=5,\n",
    "    precision=\"bf16-mixed\",\n",
    "    deterministic=True,\n",
    "    sync_batchnorm=True,\n",
    "    callbacks=[\n",
    "        pl.callbacks.RichProgressBar()\n",
    "    ],\n",
    "    logger=wandb_logger\n",
    ")\n",
    "\n",
    "trainer.fit(model=model, datamodule=dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
